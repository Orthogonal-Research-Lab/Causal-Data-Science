\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
     \usepackage[nonatbib]{neurips_2020}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\title{Using Developmental Realism and Embodiment for Neural Network Emergence}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Bradly Alicea\thanks{Use footnote for providing further information
    about author (webpage, alternative address)---\emph{not} for acknowledging
    funding agencies.} \\
  Orthogonal Research and Education Laboratory\\
  Champaign, IL 61821 \\
  \texttt{bradly.alicea@outlook.com} \\
  % examples of more authors
\And
  Rishabh Chakraborty \\
  Orthogonal Research and Education Laboratory \\
  INDIA \\
   \texttt{exynos999@outlook.com} \\
\And
  Akshara Gopi \\
  Orthogonal Research and Education Laboratory \\
  INDIA \\
   \texttt{akshdocs@gmail.com} \\
\And
  Furkan Ozcelik \\
  Orthogonal Research and Education Laboratory \\
  FRANCE \\
   \texttt{ozcelikfu@gmail.com} \\
\And
  Jesse Parent \\
  Orthogonal Research and Education Laboratory \\
  Saratoga Springs, NY \\
   \texttt{jtparent2018@gmail.com} \\
\AND

}

\begin{document}

\maketitle

\begin{abstract}
There is much to explore through synthesis of Artificial Intelligence with traditional aspects of Developmental Biology, Cognitive Science and Developmental Robotics. One lesson gleaned from this perspective is that the initialization of intelligent programs cannot solely rely on the manipulation of many parameters. One path forward is to propose a design for developmentally-inspired learning agents based on the Braitenberg Vehicle. Using these agents to exemplify artificial embodied intelligence, we move closer to modeling embodied experience and morphogenetic growth as components of cognitive capacity. We consider various factors regarding biological and cognitive development  which influence the generation of  adult phenotypes and the contingency of available developmental pathways. These mechanisms produce emergent connectivity with shifting weights and adaptive network topography, thus illustrates the importance of developmental processes in training neural networks. This approach suggests novelty in deep learning architectures might result from exploiting critical periods, embodiment, and a distinction between the assembly of neural networks and active learning on these networks.
\end{abstract}

\section{Introduction}

The process of biological development provides many novel lessons for machine learning and artificial intelligence. Often, development serves as an inspiration for artificially intelligent processes such as the serial acquisition of linguistic information [1]. Even when developmental processes are made explicit in the algorithm [2], they often focus on the psychological (particularly constructivist) nature of development [3, 4]. Less explored are explicitly developmental changes in the neural network topology [5]. Yet even in this case, such studies do not explicitly leverage embodiment as a fundamental aspect of the developmental process. Here, we use developmental Braitenberg Vehicles (dBVs) [6] to understand the developmental process via two different frames of analysis: 1) the level of network topology and 2) the impact of the external environment on agent phenotype. 

Braitenberg Vehicles [7] serve as toy models of an embodied nervous system with a simple input/output relationship between sensors and effectors. dBVs refine this in two ways: they enable an increase in representational complexity over developmental time as well as allow for multiple hidden layers between sensor and effector [8]. We will utilize the dBV approach to explore the nature of developmental learning, the role of spatial embodiment [9] in shaping the development of neural network topologies, and heterogeneous timing of information acquisition. The first aspect of this can be summarized as developmental freedom, while the third aspect is better known as the critical period of development. Collectively, developmental freedom, spatial embodiment, and critical period regulation contribute to a form of artificial learning with explicit developmental mechanisms.

\subsection{Development, Learning, and Developmental Freedom}

In dBVs, development and learning are nominally independent: development is an anatomical phenomenon, while learning is a representational one. Taken collectively, the process of a dynamic neural network defines the scope of developmental freedom, or the degrees of freedom enabled by the architecture. Developmental freedom is the tendency for information and representations in particular to take advantage of a stable underlying topology. This can be understood in relation to contingency. Developmental contingency restricts the developmental neural network to an increasingly limited set of possible trajectories. Contingency results in earlier developmental events (cell birth or the establishment of connections) to constrain the range of possibilities for future developmental events (formation of a network motif). Furthermore, contingency locks the neural network into an increasingly smaller number of pathways over developmental time, and results in path dependency. Once the network configuration becomes formalized, developmental freedom benefits from the formation of a stable circuit. 

\subsection{Spatially-dependent Embodiment}

dBVs also exhibit embodiment by sampling the incoming data through stratification, sampling the environment in a spatiotemporally-explicit manner, and the ability to compare points in a differential manner. It is on the last point (differential comparison) that allows for different frames of reference to be distinguished. In a dBV, we initialize our vehicle with sensors and effectors using a model of the genotype. The sensors can be spatialized so that they form an array of sensory inputs. This array represents different viewpoints which must be integrated into a control signal for control of the effectors. This is fundamentally distinct from the input layer of a neural network because each sensor observes the world at a specific angular perspective and position relative to the vehicle’s midline. This constant interaction with an environment of sensory cues allows the agent to keep concepts about the world continuously up to date, and provides a contextual fabric for the internal representation [8].

With respect to the latter, variation in bilateral symmetry may yield some interesting effects. In a conventional BV, bilateral symmetry is thought to allow complex movements as output. Yet when treated as distinct viewpoints of an environment, asymmetrical sensory systems can also be advantageous to learning more about the vehicle’s environment. Similarly, a dBVs overall bodily structure (a Braitenberg primitive [7]) allows it to extract more information from a single data point than would be the case than if we simply used a disembodied neural network. In short, changes in the internal relationship between sensor and effector provides context for simple behaviors.

\subsection{Network Reconfiguration and the Critical Period}

Another attribute of development is the unfolding of time-dependent processes. One powerful developmental principle we can draw from is the existence of a critical period [10, 11, 12]. Critical periods can be defined as an enhanced period of learning, which is enabled by a heightened nervous system plasticity [13]. The critical period is also an important time for learning from sensory inputs, and loss of function during this time can radically reconfigure the neural architecture [14, 15]. In the development of touch, delays in development as compared to the normal developmental trajectory can lead to a loss of sensory processing ability [16].

Hensch [10] proposes two potential mechanisms that define enhanced neural reorganization during the critical period in support of learning. First is a functional competition between inputs that might be thought of as developmental allocation. Second is a role for electrical activity in the structural consolidation of selected pathways. This has a number of consequences on the later stages of development and ultimately the adult phenotype [17]. Learning and anatomical change also seem to occur at different time scales [18], but learning in the service of behavior might be dependent on the length of time afforded to developmental growth [19].

According to this view, while both development and learning are mechanisms that induce neural and behavioral plasticity, development is a generative phenotype largely influenced by experience-expectant mechanisms. As the dBV interacts with its environment, these interactions shape the manner in which the network is generated. Learning which occurs independent of large-scale changes in the network is the product of experience-dependent influence, which is an enabling factor behind developmental freedom [20]. Changes in the timing of this transition from a neural network largely influenced by experience-expectancy to one primarily influenced by experience-dependency is key to understanding how developmental processes shape information acquisition and the supervision of learning. 

\section{Methodological Perspectives}

\subsection{Artificial Genetics and Environment}

Development is captured using a generative process of constructing nervous system nodes and weighted connections, while learning is defined by a process of connectionist plasticity. In this paper, we have instantiated these mechanisms using Genetic Algorithms and Hebbian Learning mechanisms [8]. The generalized influence of sensory information is captured using the concept of Gibsonian Information (see Appendix). 

\subsection{Potential for Reinforcement Learning}

In general, dBVs are a special case of a reinforcement learning model. Yet there are differences between developmental and reinforcement learning. The first major difference is that feedback through reinforcement is not made explicit during the developmental process. While a dBV receives environmental feedback, this is often decoupled from the expression of an internal set of processes. The dual process of generating a neural network and learning on the network could be formulated as a reinforcement learning problem (see Appendix).

\section{Analysis}

\subsection{Contingency Analysis}

Aside from being characterized by generativity and plasticity, development is also characterized by contingency. Developmental contingency [21, 22] defines the dynamic nature of dBV architecture. First, we assemble a complex neural network from an initial input/output relationship. Next, each time step is defined by the introduction of components such as an intermediate node or the formation of a new connection between two nodes with no previous direct connection. This ultimately results in a complex nervous system that is irreducible with respect to the initial condition. We hypothesize that this irreducible neural network results in a quasi-representational capacity that does not exist in a traditional BV. 

From the initial input-output mapping, every component generated by the developmental process essentially locks the network into a subset of possible future topologies; the topological evolution is an internal embodiment of path dependency. From a design perspective, our choice of critical period timing has an effect on what the Vehicle can learn and how it behaves. To understand this more clearly, we conduct a contingency analysis to better understand which types of changes lead to adaptive outcomes.

\subsection{Developmental Influences on Network Topology - Contingency Analysis}

Our contingency analysis will consider four scenarios for both the critical period and developmental freedom. Critical period learning is simulated by the rapid addition of nervous system elements relative to the rest of the developmental period. By contrast, developmental freedom is a direct consequence of critical period plasticity, and has its own signatures on the adult neural network.

\subsubsection{Early Critical Period}

When the critical period occurs early, the adult neural network tends to be more homogeneous, exhibiting the same types of configurations and motifs. Less time for preliminary or auxiliary nervous system or behavioral traits to develop; fewer varieties of experience prior to maturity.  

\subsubsection{Late Critical Period}

When the critical period occurs later, the adult neural network can exhibit a larger number of possible configurations across a population of dBVs. Slower development periods afford more time for exploration and ‘wandering’. Slower ascendance to adult/matured behavior suite/ options / capabilities. 

\subsubsection{Early Developmental Freedom}

Developmental freedom generated by an early critical period results in a low degree of overall developmental freedom. An early critical period allows for faster ascendance to a fully mature (adult) suite of behaviors suite and capabilities. 

\subsubsection{Late Developmental Freedom}

Developmental freedom generated by a late critical period results in a high degree of overall developmental freedom. Due to a later critical period, there is a slower ascendance to adult behaviors and capabilities. 

\section{Discussion}

In [8], we have previously argued that dBV architectures are low-representation networks that require other models to compute context and other, more sophisticated representations. Yet it may be that aspects of the dBVs embodied nature may afford Vehicles  a more robust  comprehension of the transactions made with their environment over developmental time [23]. We have also argued that developmental freedom, spatial embodiment, and critical period regulation play a role in learning that is missed using other types of artificial learning. The dBV approach may also be a means to balance the processing previously unseen information (unsupervised developmental learning) with the need for an innate component. 

With our toy model of an emerging nervous system, we can show how embodied neural networks are more sensitive to heterogeneous data, particularly in the spatial context. Furthermore, dBVs also teach us about the emergence of behavior as being deeply intertwined with the developmental process [24]. We can see this in the zebrafish visual system, where the maturity of representations and functions go hand in hand with developmental changes in behavior [25]. Future work will explore the potential of strategically shaping a neural network with multiple adaptive mechanisms (such as genetic and Hebbian algorithms), in addition to understanding the potential of using dBVs as a strategy for enabling complex unsupervised learning. 

\section*{Appendix}

\subsection*{Artificial Genetics and Environment}

\subsubsection*{dBV Connectivity}

This section outlines how developmental growth is implemented in a dBV as described in [7, 8]. Growth can be characterized as a form of morphological differentiation over developmental time in which structure and/or function can become increasingly complex with increases in network size and connectivity. In this case, growth is reflected in increasing the size of a network. Network growth proceeds through connectivity-activation encoding, in which the matrix {\it W(i,j)} of potential connections between intermediate nodes expands (potential connections increased) over developmental time. Expansion of the matrix represents an increase of cell number in the network, and the rate of expansion from the input-output initialization can either be deterministic or stochastic. All non-zero values represent active connections between all cells existing at a given time step. Updating {\it W(i,j)} is a discrete process governed by a genetic algorithm (for further technical details about BraGenBrain, see [7]). 

Regardless of the exact method of generating variation in cell number expansion and active connections, we still must evaluate a large number of candidate network topologies. Thus, our genetic algorithm utilizes a fitness function that is determined by movement of the vehicle relative to target stimuli. The fitness function evaluating a given dBV's {\it W(i,j)} matrix is based on stimulus-to-vehicle distance and force generated by vehicles in response to a given stimulus. Even though evolutionary variety randomizes matrix elements over time, selection nevertheless favors the emergence of networks that enable recognizable cognitive behaviors [7]. 

\subsubsection*{Mechanisms of developmental plasticity and learning}

Using a genetic algorithm as a training mechanism also generates a variety of topological orderings of matrix {\it W(i,j)} through mutation and recombination. For larger dBV neuronal networks, an encoding of coordinated projection identity can be used to more closely mimic a biological nervous system [26]. In the scheme introduced here, the critical period corresponds to a period of accelerated evolution in which the mutation rate is increased. This increases the number of potential configurations, but using a fitness criterion ensures that all of them are capable of learning and exhibiting developmental freedom post-critical period. 

At the end of the critical period, the topological mutation rate is set at near zero (or turned off completely). It has been demonstrated in [7, 8] that associative learning can occur on a stable {\it W(i,j)} matrix using a Generalized Hebbian Algorithm (GHA) [27]. Associative learning in dBVs is instantiated by building associations between sources of multiple senses simultaneously. In [7, 8], olfactory and gustatory stimuli are distributed independently in a multidimensional space. The associative learning process acts to change the connection weights in matrix {\it W(i,j)} according to the co-occurrence of the two stimuli. In this way, spatially-explicit multimodal associations can be acquired. 
\subsubsection*{Gibsonian Information as a notion of direct sensation}

Unlike a disembodied neural network, dBVs experience sensory flow as they move around their environment with respect to environmental stimuli. This combination of multiple perspectives on the reference frame and the dynamic nature of the reference frame itself involves active direct perception [28] and can be detected using something we call Gibsonian Information. Gibsonian information lies at the roots of the associative learning capacities of our network, and might be identified using quantitative measures that produce lawlike relationships [29]. 

Aside from taking advantage of changes in timing, our developmental system also exploits relative information, or the continuous difference between multiple sensors. When compared against one another, particularly during the mobile behavior of a vehicle, the current sensory state allows for the detection of coherent motion and spatial differences in a complex scene more generally. For example, we might quantify angular differences in how sensors in different locations across the front and sides of the vehicle capture the intensity or shape of a stimulus. This could not only be integrated using the GHA, but such information could also be compared to a measurement of angular momentum in the movement of the vehicle as it is receiving sensory input from the aforementioned stimulus.

\subsection*{Reinforcement Learning (RL)}

\subsubsection*{Comparison with Formal RL Models}

Our dBV model can be contrasted with the dual world/self model of [30]. In our case, we can benefit from reinforcement learning through disentangling the intrinsic and learned contributions of the developmental process on adult behaviors.

\section*{Broader Impact}
This paper contributes to a new branch of research on developmental Braitenberg Vehicles. This work is valuable in terms of both simulating the development of biological nervous systems (particularly small connectomes) and as an educational tool that advances Developmental Neuroscience instruction. This extended abstract also has a number of potential applications to data science problems, particularly ones that involve the analysis of environmental data with context. The benefits of applying developmental Braitenberg Vehicles to these types of datasets include (but are not limited to) the following issues

* the ability to operate within the "world" or context of the dataset.

* the potential of implementing a highly adaptive and embedded system on non-benchmark datasets.

The drawbacks to applying developmental Braitenberg Vehicles to these types of datasets include (but are not limited to) the following issues

* a limited understanding of the context in which the system operates.

* a limited understanding of what kinds of biases and interpretive errors can be generated using our model.

It is of note that another project in our research group deals with something called "meta-brain" models, which involve using high-representation models to address issues such as cultural context and contextual references. This type of approach might allows us to overcome some of these limitations.

\begin{ack}
There are no financial or competing interests to report.
\end{ack}

\section*{References}
\medskip
\small
[1] Saxe, A.M., McClelland, J.J., and Ganguli, S. (2019) A mathematical theory of semantic development in deep neural networks. {\it PNAS} {\bf 116}(23): 11537–11546.

[2] Lungarella, M., Mettay, G., Pfeiferz, R.,and Sandiniy, G. (2003) Developmental robotics: a survey. {\it Connection Science} {\bf 15}(4): 151–190.

[3] Smith, L.B. and Slone, L.K. (2017) A Developmental Approach to Machine Learning? {\it Frontiers in Psychology} {\bf 8}: 2124.

[4] Munakata, Y. and  McClelland, J.L. (2003) Connectionist models of development. {\it Developmental Science} {\bf 6}(4): 413–429.

[5] Kohli, M., Magoulas, G.D., and Thomas, M.S.C. (2020) Evolving Connectionist Models to Capture Population Variability across Language Development: modeling childrenʼs past tense formation. {\it Artificial Life} {\bf 26}: 217–241.

[6] Dvoretskii, S., Gong, Z., Gupta, A., Parent, J., and Alicea, B. (2020) Braitenberg Vehicles as Developmental Neurosimulation. {\it arXiv}: 2003.07689.

[7] Braitenberg, V. (1984) Vehicles: experiments in synthetic Psychology. Cambridge, MA: MIT Press.

[8] Alicea, B., Dvoretskii, S., Felder, S., Gong, Z., Gupta, A., and Parent, J. (2020) Developmental Embodied Agents as Meta-brain Models. {\it DevoNN Workshop, Artificial Life Conference}.

[9] Schettler, A., Raja, V., \\& Anderson, M.L. (2019) The Embodiment of Objects: Review, Analysis, and Future Directions. {\it Frontiers in Neuroscience} {\bf 13}: 1332.

[10] Hensch, T.K. (2004)  Critical period regulation. {\it Annual Reviews in Neuroscience} {\bf 27}: 549–579.

[11] Sengpiel, F. (2007) The critical period. {\it Current Biology}, {\bf 17}(17): R742.

[12] Frankenhuis, W.E. and Walasek, N. (2020) Modeling the evolution of sensitive periods. {\it Developmental Cognitive Neuroscience} {\bf 41}: 100715.

[13] Takesian, A.E. and Hensch, T.K. (2013) Balancing Plasticity/Stability Across Brain Development. {\it Progress in Brain Research} {\bf 207}: 3-34.

[14] Chang, E.F. and Merzenich, M.M. (2003) Environmental Noise Retards Auditory Cortical Development. {\it Science} {\bf 300}(5618): 498-502.

[15] Kahn, D.M. and Krubitzer, L. (2002) Massive cross-modal cortical plasticity and the emergence of a new cortical area in developmentally blind mammals. {\it PNAS} {\bf 99}(17): 11429-11434.

[16] Ardiel, E.L. and Rankin, C.H. (2010) The importance of touch in development. {\it Paediatric Child Health} {\bf 15}(3): 153-156.

[17] Galvan, A. (2010). Neural Plasticity of Development and Learning. {\it Human Brain Mapping} {\bf 31}: 879–890.

[18] Smith, L.B. and Thelan, E. (2003) Development as a dynamic system. {\it Trends in Cognitive Science} {\bf 7}: 343–348.

[19] Charvet, C.J. and Striedter, G.F. (2011) Developmental Modes and Developmental Mechanisms can Channel Brain Evolution. {\it Frontiers in Neuroanatomy} {\bf 5}: 4.

[20] Greenough, W.T., Black, J.E., and Wallace, C.S. (1987) Experience and Brain Development. {\it Child Development} {\bf 58}: 539-559.

[21] Oyama, S. (2000) The Ontogeny of Information: Developmental Systems and Evolution. Durham, NC: Duke University Press.

[22] Nonoyama, T. and Chiba, S. (2019) Phenotypic determinism and contingency in the evolution of hypothetical tree-like organisms. {\it PLoS One} {\bf 14}(10): e0211671.

[23] Di Paolo, E.A. (2020) Picturing Organisms and Their Environments: Interaction, Transaction, and Constitution Loops. {\it Frontiers in Psychology} {\bf 11}: 1912.

[24] Alicea, B. (2020) Raising the Connectome: the emergence of neuronal activity and behavior in C. elegans. {\it Frontiers in Cellular Neuroscience} {\bf 14}: 524791. 

[25] Avitan, L., Pujic, Z., Molter, J., McCullough, M., Zhu, S., Sun, B., Myhre1, A-E., and Goodhill, G.J. (2020) Behavioral signatures of a developing neural code. {\it Current Biology} {\bf 30}(17): 3491-3493.

[26] Li, H., Shuster, S.A., Li, J., and Luo, L. (2018) Linking neuronal lineage and wiring specificity. {\it Neural Development} {\bf 13}(5).

[27] Sanger, T.D. (1989) Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network. {\it Neural Networks} {\bf 2}: 459-473.

[28] Raja, V. (2019) J. J. Gibson’s most radical idea: The development of a new law-based psychology. {\it Theory and Psychology} {\bf 29}(6): 789-806.

[29] Gibson, J.J. (1979) The Ecological Approach to Visual Perception. Sussex, UK: Psychology Press.

[30] Haber, N., Mrowca, D., Wang, S., Fei-Fei, L., and Yamins, D.L.K. (2018). Learning to Play With Intrinsically-Motivated, Self-Aware Agents. {\it Advances in Neural Information Processing Systems 31}, pp.\ 8398–8409. Cambridge, MA: MIT Press.
\end{document}
